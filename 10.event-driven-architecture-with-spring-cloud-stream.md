# Event-driven architecture with Spring Cloud Stream

Pág. 275

---

Este capítulo trata sobre cómo diseñar e implementar nuestros microservicios basados en Spring para comunicarse con
otros microservicios mediante mensajes asincrónicos. El uso de mensajes asincrónicos para comunicarse entre aplicaciones
no es nuevo. Lo nuevo es el concepto de utilizar mensajes para comunicar eventos que representan cambios de estado. Este
concepto se llama `arquitectura basada en eventos (EDA)`. También se conoce
como `arquitectura basada en mensajes (MDA).` Lo que nos permite hacer un enfoque basado en EDA es construir sistemas
altamente desacoplados que puedan reaccionar a los cambios sin estar estrechamente acoplados a bibliotecas o servicios
específicos. Cuando se combina con microservicios, EDA nos permite agregar rápidamente nuevas funciones a nuestra
aplicación simplemente haciendo que el servicio escuche el flujo de eventos (mensajes) que emite nuestra aplicación.

El proyecto Spring Cloud hace que sea trivial crear soluciones basadas en mensajes a través del subproyecto
`Spring Cloud Stream`. `Spring Cloud Stream` nos permite implementar fácilmente la publicación y el consumo de mensajes
mientras protegemos nuestros servicios de los detalles de implementación asociados con la plataforma de mensajería
subyacente.

## 10.1 El caso de la mensajería, EDA y los microservicios

Para implementar una solución en caché, necesitamos considerar los siguientes tres requisitos básicos:

1. `Los datos almacenados en caché deben ser coherentes en todas las instancias del servicio de licencias`. Esto
   significa que no podemos almacenar en caché los datos localmente dentro del servicio de licencia porque queremos
   garantizar que se lean los mismos datos de la organización independientemente de la instancia de servicio que los
   reciba.


2. `No podemos almacenar en caché los datos de la organización en la memoria del contenedor que aloja el servicio de licencias`.
   El contenedor de tiempo de ejecución que aloja nuestro servicio suele tener un tamaño restringido y puede obtener
   datos utilizando diferentes patrones de acceso. Un caché local puede introducir complejidad porque tenemos que
   garantizar que nuestro caché local esté sincronizado con todos los demás servicios del clúster.


3. `Cuando el registro de una organización cambia mediante una actualización o eliminación, queremos que el servicio de licencias reconozca que ha habido un cambio de estado en el servicio de la organización`.
   Luego, el servicio de licencias debería invalidar cualquier dato almacenado en caché que tenga para esa organización
   específica y expulsarlo del caché.

Veamos dos enfoques para implementar estos requisitos. `El primer enfoque` implementará los requisitos establecidos
anteriormente utilizando un modelo de solicitud-respuesta sincrónica. Cuando el estado de la organización cambia, los
servicios de organización y licencias se comunicarán entre sí a través de sus puntos finales REST.

Para `el segundo enfoque`, el servicio de la organización emitirá un evento asincrónico (mensaje) para comunicar que
los datos de su organización han cambiado. Luego, el servicio de la organización publicará un mensaje en una cola, que
indicará que un registro de la organización se actualizó o eliminó: un cambio de estado. El servicio de licencias
escuchará a un intermediario (agente de mensajes o cola) para determinar si ocurrió un evento de la organización y, de
ser así, borrará los datos de la organización de su caché.

## Uso del enfoque request-response síncrono para comunicar cambios de estado

Para la caché de datos de nuestra organización, usaremos `Redis` (https://redis.io/), un almacén de `key-value`
distribuido que se utiliza como `base de datos, caché o message broker`. La `Figura 10.1` proporciona una
descripción general de alto nivel de cómo construir una solución de `almacenamiento en caché` utilizando un modelo de
programación de `request-response síncrono` tradicional como `Redis`.

![01.synchronous-request-response.png](assets/chapter-10/01.synchronous-request-response.png)

En la `figura 10.1`, cuando un usuario llama al servicio de licencias, el servicio de licencias deberá buscar los datos
de la organización. Para hacerlo, el servicio de licencias primero recuperará la organización deseada por su ID de un
clúster de Redis. Si el servicio de licencias no puede encontrar los datos de la organización, llamará al servicio de la
organización utilizando un punto final basado en REST, almacenando los datos devueltos en Redis antes de devolver los
datos de la organización al usuario.

Si alguien actualiza o elimina el registro de la organización utilizando el punto final REST del servicio de la
organización, el servicio de la organización deberá llamar a un punto final expuesto en el servicio de licencias y
decirle que invalide los datos de la organización en su caché. En la `figura 10.1`, si observamos dónde el servicio de
la organización vuelve a llamar al servicio de licencias para indicarle que invalide el caché de Redis, podemos ver al
menos tres problemas:

- Los servicios de organización y de licencias están estrechamente vinculados. Este acoplamiento introduce fragilidad
  entre los servicios.
- Si el punto final del servicio de licencias para invalidar la caché cambia, el servicio de la organización tiene que
  cambiar. Este enfoque es inflexible.
- No podemos agregar nuevos consumidores de los datos de la organización sin modificar el código en el servicio de la
  organización para verificar que llame al servicio de licencias para informarle sobre cualquier cambio.

### Acoplamiento estrecho entre servicios

Para recuperar datos, el servicio de licencia depende del servicio de la organización. Sin embargo, al hacer que el
servicio de la organización se comunique directamente con el servicio de licencias cuando se actualiza o elimina un
registro de la organización, hemos introducido el acoplamiento del servicio de la organización al servicio de
licencias (`figura 10.1`). Para que los datos en el caché de Redis se invaliden, el servicio de la organización
necesita un punto final expuesto en el servicio de licencias al que se pueda llamar para invalidar su caché de Redis, o
el servicio de la organización necesita comunicarse directamente con el servidor de Redis propiedad del servicio de
licencias. para borrar los datos que contiene.

Hacer que el servicio de la organización se comunique con Redis tiene sus propios problemas porque estamos hablando con
un almacén de datos que pertenece directamente a otro servicio. En un entorno de microservicios, esto es un gran no-no.
Si bien se puede argumentar que los datos de la organización pertenecen legítimamente al servicio de la organización, el
servicio de licencia los utiliza en un contexto específico y potencialmente podría transformar los datos o crear reglas
comerciales en torno a ellos. Si el servicio de la organización habla directamente con el servicio Redis,
accidentalmente puede infringir las reglas implementadas por el equipo propietario del servicio de licencias.

### Fragilidad entre los servicios

El estrecho vínculo entre el servicio de licencias y el servicio de organización también introdujo fragilidad entre los
dos servicios. Si el servicio de licencias no funciona o funciona con lentitud, el servicio de la organización puede
verse afectado ya que el servicio de la organización ahora se comunica directamente con el servicio de licencias.
Nuevamente, si el servicio de la organización se comunica directamente con el almacén de datos de Redis del servicio de
licencias, creamos una dependencia entre el servicio de la organización y Redis. En este escenario, cualquier problema
con el servidor Redis compartido ahora tiene el potencial de desactivar ambos servicios.

### Inflexibilidad para sumar nuevos consumidores a los cambios en el servicio de la organización

Con el modelo de la `figura 10.1`, si tuviéramos otro servicio interesado en los cambios de datos de la organización,
necesitaríamos agregar otra llamada del servicio de la organización al otro servicio. Esto significa un cambio de código
y una nueva implementación del servicio de la organización, lo que puede introducir un estado de inflexibilidad en
nuestro código.

Si utilizamos el modelo sincrónico de solicitud-respuesta para comunicar el cambio de estado, comenzamos a ver un patrón
de dependencia similar a una web entre nuestros servicios principales en nuestra aplicación y otros servicios. Los
centros de estas webs se convierten en nuestros principales puntos de fallo dentro de nuestra aplicación.

## Uso de mensajería para comunicar cambios de estado entre servicios

Vamos a inyectar un `topic` entre el servicio de licencias y organización con un enfoque de mensajería. El sistema de
mensajería no se utilizará para leer datos del servicio de la organización, sino que el servicio de la organización lo
utilizará para publicar cualquier cambio de estado dentro de los datos administrados por el servicio de la organización
cuando estos ocurran. La `figura 10.2` demuestra este enfoque.

![02.messages-queue.png](assets/chapter-10/02.messages-queue.png)

En el modelo de la `figura 10.2`, cuando los datos de la organización cambian, el servicio de la organización publica un
mensaje en un topic. El servicio de licencias monitorea el topic en busca de mensajes y, cuando llega un mensaje, borra
el registro de la organización correspondiente de la caché de Redis. Cuando se trata del estado de comunicación, la cola
de mensajes actúa como intermediaria entre los servicios de licencia y organización. Este enfoque ofrece cuatro
beneficios: acoplamiento flexible, durabilidad, escalabilidad y flexibilidad.

### Bajo acoplamiento

Una aplicación de microservicios puede estar compuesta por docenas de servicios pequeños y distribuidos que interactúan
entre sí y con los datos administrados entre sí. Como vimos con el diseño síncrono propuesto anteriormente, una
respuesta HTTP síncrona crea una fuerte dependencia entre los servicios de licencia y de organización. No podemos
eliminar estas dependencias por completo, pero podemos intentar minimizarlas exponiendo solo los puntos finales que
administran directamente los datos propiedad del servicio.

Un enfoque de mensajería nos permite desacoplar los dos servicios porque, cuando se trata de comunicar cambios de
estado, ninguno de los servicios conoce al otro. Cuando el servicio de la organización necesita publicar un cambio de
estado, escribe un mensaje en una cola. El servicio de licencias sólo sabe que recibe un mensaje; no tiene idea quien ha
publicado el mensaje.

### Durabilidad

La presencia de la cola nos permite garantizar que se entregará un mensaje incluso si el consumidor del servicio está
caído. Por ejemplo, el servicio de la organización puede seguir publicando mensajes incluso si el servicio de licencia
no está disponible. Los mensajes se almacenan en la cola y permanecen allí hasta que el servicio de licencia esté
disponible. Por el contrario, con la combinación de caché y cola, si el servicio de la organización no funciona, el
servicio de licencias puede degradarse sin problemas porque al menos parte de los datos de la organización estarán en su
caché. A veces, los datos antiguos son mejores que ningún dato.

### Escalabilidad

Debido a que los mensajes se almacenan en una cola, el remitente del mensaje no tiene que esperar una respuesta del
consumidor del mensaje. El remitente puede seguir su camino y seguir trabajando. Del mismo modo, si un consumidor que
lee un mensaje de la cola no procesa los mensajes lo suficientemente rápido, es una tarea trivial atraer a más
consumidores y hacer que procesen los mensajes. Este enfoque de escalabilidad encaja bien dentro de un modelo de
microservicios.

Una de las cosas que hemos enfatizado a lo largo de este libro es que debería ser trivial generar nuevas instancias de
un microservicio. Luego, el microservicio adicional puede convertirse en otro servicio para procesar la cola de
mensajes. Este es un ejemplo de escalamiento horizontal.

Los mecanismos de escalado tradicionales para leer mensajes en una cola implicaban aumentar la cantidad de subprocesos
que un consumidor de mensajes podía procesar al mismo tiempo. Desafortunadamente, con este enfoque, en última instancia
estábamos limitados por la cantidad de CPU disponibles para el consumidor de mensajes. Un modelo de microservicio no
tiene esta limitación porque podemos escalar aumentando la cantidad de máquinas que alojan el servicio y consumen los
mensajes.

### Flexibilidad

El remitente de un mensaje no tiene idea de quién lo va a consumir. Esto significa que podemos agregar fácilmente nuevos
consumidores de mensajes (y nuevas funciones) sin afectar el servicio de envío original. Este es un concepto
extremadamente poderoso porque se pueden agregar nuevas funciones a una aplicación sin tener que tocar los servicios
existentes. En cambio, el nuevo código puede escuchar los eventos que se publican y reaccionar en consecuencia.


